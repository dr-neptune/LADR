* Eigenvalues, Eigenvectors, and Invariant Subspaces 

Linear maps from one vector space to another vector space were the objects of study in chapter 3. Now we investigate linear maps from a finite-dimensional vector space to itself. 

Learning Objectives: 
- Invariant Subspaces
- Eigenvalues, Eigenvectors, Eigenspaces 
- Each operator on a fine-dimensional complex vector space has an eigenvalue and an upper-triangular matrix with respect to some basis 

** 5.A | Invariant Subspaces 

The notion of a subspace that gets mapped into itself is sufficiently important to deserve a name

*** Definition | Invariant Subspace

Suppose $T \in \mathcal{L}(V)$. A subspace $U$ of $V$ is called invariant under $T$ if $u \in U$ implies $Tu \in U$. 

In other words, $U$ is invariant under $T$ if $T|_U$ is an operator on $U$

The equation $Tv = \lambda v$ is important enough that the vectors $v$ and scalars $\lambda$ satisfying it are given special names: 

*** Definition | Eigenvalue 

Suppose $T \in \mathcal{L}(V)$. A number $\lambda \in F$ is called an eigenvalue of $T$ if there exists $v \in V$ such that $v \neq 0$ and $Tv = \lambda v$. 

*** Theorem | Equivalent Conditions to be an Eigenvalue 

Suppose $V$ is finite-dimensional, $T \in \mathcal{L}(V)$, and $\lambda \in F$. Then the following are equivalent 

- $\lambda$ is an eigenvalue of $T$
- $T - \lambda I$ is not injective
- $T - \lambda I$ is not surjective
- $T - \lambda I$ is not invertible

*** Definition | Eigenvector 

Suppose $T \in \mathcal{L}(V)$ and $\lambda \in F$ is an eigenvalue of $T$. A vector $v \in V$ is called an eigenvector of $T$ corresponding to $\lambda$ if $v \neq 0$ and $Tv = \lambda v$. 

*** Theorem | Linearly Independent Eigenvectors 

Let $T \in \mathcal{L}(V)$. Suppose $\lambda_1, ..., \lambda_m$ are distinct eigenvalues of $T$ and $v_1, ..., v_m$ are corresponding eigenvectors. Then $v_1, ..., v_m$ is linearly independent. 

*** Theorem | Number of Eigenvalues 

Suppose $V$ is finite-dimensional. Then each operator on $V$ has at most dim $V$ distinct eigenvalues. 

*** Definition | $T|_U$ and $T/U$

Suppose $T \in \mathcal{L}(V)$ and $U$ is a subspace of $V$ invariant under $T$

- The restriction operator $T|_U \in \mathcal{L}(U)$ is defined by $T|_U (u) = Tu$ for $u \in U$
- The quotient operator $T/U \in \mathcal{L}(V/U)$ is defined by $(T/U)(v + U) = Tv + U$ for $v \in V$ 


** 5.B | Eigenvectors and Upper-Triangular Matrices 

The main reason that a richer theory ecists for operators (which map a vector space onto itself) than for more general linear maps is that operators can be raised to powers. We begin this section by defining that notion and the key concept of applying a polynomial to an operator. 

*** Definition | $T^m$

Suppose $T \in \mathcal{L}(V)$ and $m$ is a positive integer. 

- $T^m$ is defined by $T^m = T ... T$
- $T^0$ is defined to be the identity operator $I$ on $V$
- If $T$ is invertible with inverse $T^{-1}$, then $T^{-m}$ is defined by $(T^{-1})^m$

Similiarly,

- $T^m T^n = T^{m + n}$
- $(T^m)^n = T^{mn}$

*** Definition | $p(T)$

Suppose $T \in \mathcal{L}(V)$ and $p \in \mathcal{P}(F)$ is a polynomial given by $p(z) = a_0 + a_1 z + a_2 z^2 + ... + a_m z^m$ for $z \in F$. Then $p(T)$ is the operator defined by $p(T) = a_0 I + a_1 T + a_2 T^2 + ... + a_m T^m$.

This is just a new use of the symbol $p$ because we are applying it to operators, not just elements of $F$. 

*** Definition | Product of Polynomials 

If $p, q \in \mathcal{P}(F)$, then $pq \in \mathcal{P}(F)$ is the polynomial defined by $(pq)(z) = p(z)q(z)$ for $z \in F$. 

*** Theorem | Multiplicative Properties 

Suppose $p, q \in \mathcal{P}(F)$ and $T \in \mathcal{L}(V)$. Then 
- $(pq)(T) = p(T)q(T)$
- $p(T)q(T) = q(T)p(T)$


Now we come to one of the central results about operators on complex vector spaces

*** Theorem | Operators on Complex Vector Spaces have an Eigenvalue 

Every operator on a finite-dimensional, nonzero, complex vector space has an eigenvalue 

*** Definition | matrix of an operator, $\mathcal{M}(T)$

Suppose $T \in \mathcal{L}(V)$ and $v_1, ..., v_n$ is a basis of $V$. The matrix of $T$ with respect to this basis is the n-by-n matrix 

$\mathcal{M}(T) =  \begin{pmatrix} A_{1, 1} & \cdots & A_{1, n} \\ \vdots &  & \vdots \\ A_{n, 1} & \cdots & A_{n, n} \end{pmatrix}$


whose entries $A_{j, k}$ are defined by $Tv_k = A_{1, k} v_1 + ... + A_{n, k} v_n$

If the basis is not clear from the context, then the notation $\mathcal{M}(T, (v_1, ..., v_n))$ is used. 

*** Definition | Diagonal of a Matrix 

The diagonal of a square matrix consists of the entries along the line from the upper left corner to the bottom right corner 

*** Definition | Upper-Triangular Matrix 

A matrix is called upper triangular if all the entries below the diagonal equal 0 

*** Theorem | Conditions for Upper-Triangular Matrix 

Suppose $T \in \mathcal{L}(V)$ and $v_1, ..., v_n$ is a basis of $V$. Then the following are equivalent 

- the matrix of $T$ with respect to $v_1, ..., v_n$ is upper triangular
- $Tv_j \in$ span$(v_1, ..., v_j)$ for each $j = 1, ..., n$
- span$(v_1, ..., v_j)$ is invariant under $T$ for each $j = 1, ..., n$


